\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {第一章\hspace  {0.3em}}序论}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1988learning}
\citation{hinton2006reducing}
\citation{hinton2012deep}
\citation{krizhevsky2012imagenet}
\citation{Goodfellow-et-al-2015-Book}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}进一步的阅读和总结}{9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第二章\hspace  {0.3em}}数学准备知识}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}矢量分析}{11}}
\newlabel{2.1}{{2.1}{11}}
\newlabel{2.2}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}矢量的模}{11}}
\newlabel{2.3}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}矢量的范数}{11}}
\newlabel{2.4}{{2.4}{12}}
\newlabel{2.5}{{2.5}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}矩阵及其基本运算}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}常见的矩阵}{12}}
\newlabel{2.6}{{2.6}{12}}
\newlabel{2.7}{{2.7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}矩阵的范数}{12}}
\newlabel{compatibility}{{2.8}{12}}
\newlabel{2.8}{{2.9}{12}}
\newlabel{UnitaryInvariance}{{2.10}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}导数}{13}}
\newlabel{div}{{2.11}{13}}
\newlabel{2.9}{{2.12}{13}}
\newlabel{2.10}{{2.13}{14}}
\newlabel{divmat}{{2.14}{14}}
\newlabel{innerdivmat}{{2.15}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}常见的向量导数}{14}}
\newlabel{ex1}{{2.16}{14}}
\newlabel{ex2}{{2.17}{14}}
\newlabel{ex3}{{2.18}{14}}
\newlabel{2.11}{{2.19}{14}}
\newlabel{2.12}{{2.20}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}导数法则}{14}}
\newlabel{2.13}{{2.21}{15}}
\newlabel{2.14}{{2.22}{15}}
\newlabel{2.15}{{2.23}{15}}
\newlabel{2.16}{{2.24}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}常用函数}{15}}
\newlabel{2.17}{{2.25}{15}}
\newlabel{2.18}{{2.26}{15}}
\newlabel{2.19}{{2.27}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}logistic/sigmoid函数}{16}}
\newlabel{2.20}{{2.28}{16}}
\newlabel{2.21}{{2.29}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}softmax函数}{16}}
\newlabel{2.22}{{2.30}{16}}
\newlabel{2.23}{{2.31}{16}}
\newlabel{2.24}{{2.32}{16}}
\newlabel{2.27}{{2.35}{17}}
\newlabel{2.30}{{2.38}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}一些练习}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}进一步的阅读和总结}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第三章\hspace  {0.3em}}机器学习概述}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}机器学习概述}{19}}
\newlabel{3.1}{{3.1}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces 机器学习系统示意图}}{19}}
\newlabel{fig:3.1}{{3.1}{19}}
\citation{mitchell1998introduction}
\newlabel{3.2}{{3.2}{20}}
\newlabel{3.3}{{3.3}{20}}
\newlabel{3.4}{{3.4}{20}}
\newlabel{3.5}{{3.5}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}损失函数}{21}}
\newlabel{3.7}{{3.7}{21}}
\newlabel{3.8}{{3.8}{21}}
\newlabel{3.9}{{3.9}{21}}
\newlabel{3.10}{{3.10}{21}}
\citation{principe2000information}
\newlabel{3.11}{{3.11}{22}}
\newlabel{3.12}{{3.12}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}补充问题：机器学习与信息论的关系}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Some information formulas and their properties as learning measures}}{22}}
\newlabel{fig:3.2}{{3.2}{22}}
\citation{mackay2003information}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The relationship among some measures}}{23}}
\newlabel{fig:3.3}{{3.3}{23}}
\citation{hu2015information}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The relationship among $E,Rej,A,CR$}}{24}}
\newlabel{fig:3.4}{{3.4}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Example}}{25}}
\newlabel{fig:3.5}{{3.5}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}机器学习算法的类型}{25}}
\newlabel{3.13}{{3.13}{25}}
\newlabel{3.14}{{3.14}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}机器学习中的一些基本概念}{26}}
\newlabel{3.15}{{3.15}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}参数学习方法}{27}}
\newlabel{3.16}{{3.16}{28}}
\newlabel{3.17}{{3.17}{28}}
\newlabel{3.19}{{3.19}{28}}
\newlabel{3.20}{{3.21}{28}}
\citation{rumelhart1988learning}
\citation{duchi2011adaptive}
\citation{zeiler2012adadelta}
\newlabel{3.21}{{3.22}{29}}
\newlabel{3.22}{{3.23}{29}}
\newlabel{3.23}{{3.24}{29}}
\newlabel{3.24}{{3.25}{30}}
\newlabel{3.25}{{3.26}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}线性回归}{30}}
\newlabel{3.26}{{3.27}{30}}
\newlabel{3.27}{{3.28}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}线性分类}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}二类分类}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}多类线性分类}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}评价方法}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}进一步的阅读和总结}{30}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第四章\hspace  {0.3em}}感知器}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}二类感知器}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}感知器学习算法}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}线性感知器收敛性证明}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}多类感知器}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}多类感知器收敛性证明}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}投票感知器}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}进一步的阅读和总结}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第五章\hspace  {0.3em}}人工神经网络}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}神经元}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}激活函数}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}前馈神经网络}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}前馈计算}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}反向传播算法}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}梯度消失问题}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}训练方法}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}一些经验}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}进一步的阅读和总结}{33}}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {第六章\hspace  {0.3em}}受限波尔兹曼机RBM}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Roadmap}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Notations}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Energy-Based Models}{36}}
\newlabel{add1.1}{{6.1}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces An Simple Example of Energy-Based Models}}{37}}
\newlabel{fig:add1}{{6.1}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}An Simple Example of Energy-Based Models}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces $Energy(a,b)$ and the probability for each configuration}}{37}}
\newlabel{tab:Energy}{{6.1}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Introducing Hidden Variables}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Model}}{38}}
\newlabel{fig:add2}{{6.2}{38}}
\newlabel{add1.2}{{6.2}{38}}
\newlabel{add1.3}{{6.3}{38}}
\newlabel{add1.4}{{6.4}{38}}
\newlabel{add1.5}{{6.5}{38}}
\newlabel{add1.6}{{6.6}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Gradient Learning of Energy-based Models}}{39}}
\newlabel{fig:add3}{{6.3}{39}}
\newlabel{add1.7}{{6.7}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Gradient Learning of Energy-based Models}{39}}
\newlabel{add1.8}{{6.8}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Boltzmann Machines}{40}}
\newlabel{BME}{{6.9}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Gradient Learning of Boltzmann Machines}{40}}
\newlabel{LLG}{{6.10}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Gibbs Sampling for Conditional Probability}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Gibbs Sampling for Boltzmann Machines}{41}}
\citation{hinton2002training}
\citation{welling2007product}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Restricted Boltzmann Machines}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Gibbs Sampling for Restricted Boltzmann Machines}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {6.13}Contrastive Divergence}{43}}
\newlabel{CDk}{{6.11}{44}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces k-step contrastive divergence}}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.14}Gibbs Sampling和Markov Chain以及MCMC的关系}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {6.15}CD Algorithm是如何对原来的分布$p$进行优化的}{46}}
\citation{hubel1968receptive}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {chapter}{\numberline {第七章\hspace  {0.3em}}卷积神经网络CNN}{49}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}卷积}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}一维场合}{49}}
\newlabel{6.1}{{7.1}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Full Connection Layer and Convolutional Layer}}{50}}
\newlabel{fig:6.1}{{7.1}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}二维场合}{50}}
\newlabel{6.2}{{7.2}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}卷积层：用卷积代替全链接}{50}}
\newlabel{6.2}{{7.3}{51}}
\newlabel{6.5}{{7.5}{51}}
\newlabel{6.6}{{7.6}{51}}
\newlabel{6.7}{{7.7}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The mapping relationship of 2-D convolutional layer}}{52}}
\newlabel{fig:6.2}{{7.2}{52}}
\newlabel{6.8}{{7.8}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces 2-D convolutional layer}}{53}}
\newlabel{fig:6.3}{{7.3}{53}}
\newlabel{6.9}{{7.9}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}子采样层：池化}{53}}
\newlabel{6.10}{{7.10}{53}}
\citation{lecun1998gradient}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Net Structure of LeNet-5}}{54}}
\newlabel{fig:6.4}{{7.4}{54}}
\newlabel{6.12}{{7.12}{54}}
\newlabel{6.14}{{7.14}{54}}
\newlabel{6.15}{{7.15}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}CNN示例：LeNet-5}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Connection table of LeNet-5's C3 layer}}{55}}
\newlabel{fig:6.5}{{7.5}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}梯度计算}{56}}
\newlabel{6.16}{{7.16}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}卷积层的梯度}{56}}
\newlabel{6.18}{{7.18}{56}}
\newlabel{6.22}{{7.22}{57}}
\newlabel{6.24}{{7.24}{57}}
\newlabel{6.25}{{7.25}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}子采样层的梯度}{57}}
\newlabel{6.26}{{7.26}{57}}
\newlabel{6.27}{{7.27}{57}}
\newlabel{6.30}{{7.30}{58}}
\newlabel{6.31}{{7.31}{58}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}一个强大的CNN框架：CAFFE}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Caffe的特点}{58}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe相对与其他DL框架的优点和缺点}{58}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe代码层次}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}更进一步的特点}{59}}
\@writefile{toc}{\contentsline {subsubsection}{Blob}{59}}
\@writefile{toc}{\contentsline {subsubsection}{Layer}{59}}
\@writefile{toc}{\contentsline {subsubsection}{Net}{61}}
\@writefile{toc}{\contentsline {subsubsection}{Solver}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}一些关于CNN的技巧}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Data Augmentation}{62}}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Pre-Processing}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Initializations}{65}}
\citation{he2015delving}
\citation{he2015delving}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}During Training}{67}}
\citation{gatys2015neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.5}Activation Functions}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.6}Regularizations}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.7}Insights from Figures}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.8}Ensemble}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.9}Miscellaneous}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}一些经典论文基于CAFFE的实验重现}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}A Neural Algorithm of Artistic Style}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}进一步的阅读和总结}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Images that combine the content of a photograph with the style of several well-known artworks.}}{70}}
\newlabel{fig:6.6}{{7.6}{70}}
\citation{waibel1989phoneme}
\citation{werbos1990backpropagation}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\citation{hochreiter2001gradient}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {chapter}{\numberline {第八章\hspace  {0.3em}}递归神经网络RNN}{71}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{7.1}{{8.1}{71}}
\citation{elman1990finding}
\citation{werbos1990backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces RNN}}{72}}
\newlabel{fig:7.1}{{8.1}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}简单的递归网络}{72}}
\newlabel{7.2}{{8.2}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}梯度}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Simple RNN expanded by time}}{73}}
\newlabel{fig:7.2}{{8.2}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces RNN expanded by time}}{73}}
\newlabel{fig:7.3}{{8.3}{73}}
\newlabel{7.3}{{8.3}{73}}
\newlabel{7.5}{{8.5}{73}}
\newlabel{7.6}{{8.6}{73}}
\newlabel{7.8}{{8.8}{74}}
\newlabel{7.9}{{8.9}{74}}
\newlabel{7.10}{{8.10}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}改进方案}{74}}
\newlabel{7.11}{{8.11}{74}}
\citation{hochreiter1997long}
\citation{hochreiter1997long}
\citation{sutskever2014sequence}
\newlabel{7.12}{{8.12}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}长短时记忆神经网络：LSTM}{75}}
\newlabel{7.14}{{8.14}{75}}
\citation{cho2014learning}
\citation{chung2014empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces LSTM Structure Expamle}}{76}}
\newlabel{fig:7.4}{{8.4}{76}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}门限循环单元：GRU}{76}}
\bibstyle{plain}
\bibdata{Lect}
\newlabel{7.20}{{8.20}{77}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}一个强大的RNN框架：DeepNet}{77}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}一些经典论文基于DeepNet的实验重现}{77}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}进一步的阅读和总结}{77}}
\bibcite{bengio2009learning}{1}
\bibcite{bengio1994learning}{2}
\bibcite{cho2014learning}{3}
\bibcite{chung2014empirical}{4}
\bibcite{duchi2011adaptive}{5}
\bibcite{elman1990finding}{6}
\bibcite{gatys2015neural}{7}
\bibcite{Goodfellow-et-al-2015-Book}{8}
\bibcite{he2015delving}{9}
\bibcite{hinton2012deep}{10}
\bibcite{hinton2002training}{11}
\bibcite{hinton2006reducing}{12}
\bibcite{hochreiter2001gradient}{13}
\bibcite{hochreiter1997long}{14}
\bibcite{hu2015information}{15}
\bibcite{hubel1968receptive}{16}
\bibcite{krizhevsky2012imagenet}{17}
\bibcite{lecun1998gradient}{18}
\bibcite{mackay2003information}{19}
\bibcite{mitchell1998introduction}{20}
\bibcite{principe2000information}{21}
\bibcite{rosenblatt1958perceptron}{22}
\bibcite{rumelhart1988learning}{23}
\bibcite{sutskever2014sequence}{24}
\bibcite{waibel1989phoneme}{25}
\bibcite{welling2007product}{26}
\bibcite{werbos1990backpropagation}{27}
\bibcite{zeiler2012adadelta}{28}
