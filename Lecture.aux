\relax 
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第一章\hspace  {0.3em}}序论}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1988learning}
\citation{hinton2006reducing}
\citation{hinton2012deep}
\citation{krizhevsky2012imagenet}
\citation{Goodfellow-et-al-2015-Book}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}进一步的阅读和总结}{9}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第二章\hspace  {0.3em}}数学准备知识}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}矢量分析}{11}}
\newlabel{2.1}{{2.1}{11}}
\newlabel{2.2}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}矢量的模}{11}}
\newlabel{2.3}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}矢量的范数}{11}}
\newlabel{2.4}{{2.4}{12}}
\newlabel{2.5}{{2.5}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}矩阵及其基本运算}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}常见的矩阵}{12}}
\newlabel{2.6}{{2.6}{12}}
\newlabel{2.7}{{2.7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}矩阵的范数}{12}}
\newlabel{compatibility}{{2.8}{12}}
\newlabel{2.8}{{2.9}{12}}
\newlabel{UnitaryInvariance}{{2.10}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}导数}{13}}
\newlabel{div}{{2.11}{13}}
\newlabel{2.9}{{2.12}{13}}
\newlabel{2.10}{{2.13}{14}}
\newlabel{divmat}{{2.14}{14}}
\newlabel{innerdivmat}{{2.15}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}常见的向量导数}{14}}
\newlabel{ex1}{{2.16}{14}}
\newlabel{ex2}{{2.17}{14}}
\newlabel{ex3}{{2.18}{14}}
\newlabel{2.11}{{2.19}{14}}
\newlabel{2.12}{{2.20}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}导数法则}{14}}
\newlabel{2.13}{{2.21}{15}}
\newlabel{2.14}{{2.22}{15}}
\newlabel{2.15}{{2.23}{15}}
\newlabel{2.16}{{2.24}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}常用函数}{15}}
\newlabel{2.17}{{2.25}{15}}
\newlabel{2.18}{{2.26}{15}}
\newlabel{2.19}{{2.27}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}logistic/sigmoid函数}{16}}
\newlabel{2.20}{{2.28}{16}}
\newlabel{2.21}{{2.29}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}softmax函数}{16}}
\newlabel{2.22}{{2.30}{16}}
\newlabel{2.23}{{2.31}{16}}
\newlabel{2.24}{{2.32}{16}}
\newlabel{2.27}{{2.35}{17}}
\newlabel{2.30}{{2.38}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Rectifier函数(ReLU)}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}一些练习}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}进一步的阅读和总结}{18}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第三章\hspace  {0.3em}}机器学习概述}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}机器学习概述}{19}}
\newlabel{3.1}{{3.1}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces 机器学习系统示意图}}{19}}
\newlabel{fig:3.1}{{3.1}{19}}
\citation{mitchell1998introduction}
\newlabel{3.2}{{3.2}{20}}
\newlabel{3.3}{{3.3}{20}}
\newlabel{3.4}{{3.4}{20}}
\newlabel{3.5}{{3.5}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Overfit}}{21}}
\newlabel{fig:Regu1}{{3.2}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}补充问题：正则化}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Dropout}}{25}}
\newlabel{fig:Regu2}{{3.3}{25}}
\citation{simard2003best}
\citation{sun2014deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}损失函数}{27}}
\newlabel{3.7}{{3.7}{27}}
\newlabel{3.8}{{3.8}{27}}
\newlabel{3.9}{{3.9}{27}}
\newlabel{3.10}{{3.10}{27}}
\newlabel{3.11}{{3.11}{27}}
\citation{principe2000information}
\newlabel{3.12}{{3.12}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}补充问题：机器学习与信息论的关系}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Some information formulas and their properties as learning measures}}{28}}
\newlabel{fig:3.2}{{3.4}{28}}
\citation{mackay2003information}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The relationship among some measures}}{29}}
\newlabel{fig:3.3}{{3.5}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The relationship among $E,Rej,A,CR$}}{29}}
\newlabel{fig:3.4}{{3.6}{29}}
\citation{hu2015information}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}机器学习算法的类型}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Example}}{31}}
\newlabel{fig:3.5}{{3.7}{31}}
\newlabel{3.13}{{3.13}{31}}
\newlabel{3.14}{{3.14}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}机器学习中的一些基本概念}{32}}
\newlabel{3.15}{{3.15}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}参数学习方法}{33}}
\newlabel{3.16}{{3.16}{33}}
\newlabel{3.17}{{3.17}{34}}
\newlabel{3.19}{{3.19}{34}}
\newlabel{3.20}{{3.21}{34}}
\citation{rumelhart1988learning}
\citation{duchi2011adaptive}
\citation{zeiler2012adadelta}
\newlabel{3.21}{{3.22}{35}}
\newlabel{3.22}{{3.23}{35}}
\newlabel{3.23}{{3.24}{35}}
\newlabel{3.24}{{3.25}{35}}
\newlabel{3.25}{{3.26}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}线性回归}{36}}
\newlabel{3.26}{{3.27}{36}}
\newlabel{3.27}{{3.28}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}线性分类}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}二类分类}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}多类线性分类}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}评价方法}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}进一步的阅读和总结}{36}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第四章\hspace  {0.3em}}感知器}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}二类感知器}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}感知器学习算法}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}线性感知器收敛性证明}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}多类感知器}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}多类感知器收敛性证明}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}投票感知器}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}进一步的阅读和总结}{37}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {chapter}{\numberline {第五章\hspace  {0.3em}}人工神经网络}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}神经元}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}激活函数}{40}}
\citation{glorot2010understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Neuron}}{41}}
\newlabel{fig:5.1}{{5.1}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}激活函数的表达能力}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Activation Function}}{42}}
\newlabel{fig:5.2}{{5.2}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces $s$-Volume}}{45}}
\newlabel{fig:5.3}{{5.3}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}前馈神经网络}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}前馈计算}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}反向传播算法}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}梯度消失问题}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}训练方法}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}一些经验}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}进一步的阅读和总结}{45}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {第六章\hspace  {0.3em}}受限波尔兹曼机RBM}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Roadmap}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Notations}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Energy-Based Models}{48}}
\newlabel{add1.1}{{6.1}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces An Simple Example of Energy-Based Models}}{49}}
\newlabel{fig:add1}{{6.1}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}An Simple Example of Energy-Based Models}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces $Energy(a,b)$ and the probability for each configuration}}{49}}
\newlabel{tab:Energy}{{6.1}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Introducing Hidden Variables}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Model}}{50}}
\newlabel{fig:add2}{{6.2}{50}}
\newlabel{add1.2}{{6.2}{50}}
\newlabel{add1.3}{{6.3}{50}}
\newlabel{add1.4}{{6.4}{50}}
\newlabel{add1.5}{{6.5}{50}}
\newlabel{add1.6}{{6.6}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Gradient Learning of Energy-based Models}}{51}}
\newlabel{fig:add3}{{6.3}{51}}
\newlabel{add1.7}{{6.7}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Gradient Learning of Energy-based Models}{51}}
\newlabel{add1.8}{{6.8}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Boltzmann Machines}{52}}
\newlabel{BME}{{6.9}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Gradient Learning of Boltzmann Machines}{52}}
\newlabel{LLG}{{6.10}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Gibbs Sampling for Conditional Probability}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Gibbs Sampling for Boltzmann Machines}{53}}
\citation{hinton2002training}
\citation{welling2007product}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Restricted Boltzmann Machines}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Gibbs Sampling for Restricted Boltzmann Machines}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {6.13}Contrastive Divergence}{55}}
\newlabel{CDk}{{6.11}{56}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces k-step contrastive divergence}}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {6.14}Gibbs Sampling和Markov Chain以及MCMC的关系}{58}}
\@writefile{toc}{\contentsline {section}{\numberline {6.15}CD Algorithm是如何对原来的分布$p$ 进行优化的}{58}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{hubel1968receptive}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {chapter}{\numberline {第七章\hspace  {0.3em}}卷积神经网络CNN}{61}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}卷积}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}一维场合}{61}}
\newlabel{6.1}{{7.1}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Full Connection Layer and Convolutional Layer}}{62}}
\newlabel{fig:6.1}{{7.1}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}二维场合}{62}}
\newlabel{6.2}{{7.2}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}卷积层：用卷积代替全链接}{62}}
\newlabel{6.2}{{7.3}{63}}
\newlabel{6.5}{{7.5}{63}}
\newlabel{6.6}{{7.6}{63}}
\newlabel{6.7}{{7.7}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The mapping relationship of 2-D convolutional layer}}{64}}
\newlabel{fig:6.2}{{7.2}{64}}
\newlabel{6.8}{{7.8}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces 2-D convolutional layer}}{65}}
\newlabel{fig:6.3}{{7.3}{65}}
\newlabel{6.9}{{7.9}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}子采样层：池化}{65}}
\newlabel{6.10}{{7.10}{65}}
\citation{lecun1998gradient}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Net Structure of LeNet-5}}{66}}
\newlabel{fig:6.4}{{7.4}{66}}
\newlabel{6.12}{{7.12}{66}}
\newlabel{6.14}{{7.14}{66}}
\newlabel{6.15}{{7.15}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}CNN示例：LeNet-5}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Connection table of LeNet-5's C3 layer}}{67}}
\newlabel{fig:6.5}{{7.5}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}梯度计算}{68}}
\newlabel{6.16}{{7.16}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}卷积层的梯度}{68}}
\newlabel{6.18}{{7.18}{68}}
\newlabel{6.22}{{7.22}{69}}
\newlabel{6.24}{{7.24}{69}}
\newlabel{6.25}{{7.25}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}子采样层的梯度}{69}}
\newlabel{6.26}{{7.26}{69}}
\newlabel{6.27}{{7.27}{69}}
\newlabel{6.30}{{7.30}{70}}
\newlabel{6.31}{{7.31}{70}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}一个强大的CNN框架：CAFFE}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Caffe的特点}{70}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe相对与其他DL框架的优点和缺点}{70}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe代码层次}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}更进一步的特点}{71}}
\@writefile{toc}{\contentsline {subsubsection}{Blob}{71}}
\@writefile{toc}{\contentsline {subsubsection}{Layer}{71}}
\@writefile{toc}{\contentsline {subsubsection}{Net}{73}}
\@writefile{toc}{\contentsline {subsubsection}{Solver}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}一些关于CNN的技巧}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Data Augmentation}{74}}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Pre-Processing}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Initializations}{77}}
\citation{he2015delving}
\citation{he2015delving}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}During Training}{79}}
\citation{gatys2015neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.5}Activation Functions}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.6}Regularizations}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.7}Insights from Figures}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.8}Ensemble}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.9}Miscellaneous}{80}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}一些经典论文基于CAFFE的实验重现}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}A Neural Algorithm of Artistic Style}{80}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}进一步的阅读和总结}{81}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Images that combine the content of a photograph with the style of several well-known artworks.}}{82}}
\newlabel{fig:6.6}{{7.6}{82}}
\citation{waibel1989phoneme}
\citation{werbos1990backpropagation}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\citation{hochreiter2001gradient}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {chapter}{\numberline {第八章\hspace  {0.3em}}递归神经网络RNN}{83}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{7.1}{{8.1}{83}}
\citation{elman1990finding}
\citation{werbos1990backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces RNN}}{84}}
\newlabel{fig:7.1}{{8.1}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}简单的递归网络}{84}}
\newlabel{7.2}{{8.2}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}梯度}{84}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Simple RNN expanded by time}}{85}}
\newlabel{fig:7.2}{{8.2}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces RNN expanded by time}}{85}}
\newlabel{fig:7.3}{{8.3}{85}}
\newlabel{7.3}{{8.3}{85}}
\newlabel{7.5}{{8.5}{85}}
\newlabel{7.6}{{8.6}{85}}
\newlabel{7.8}{{8.8}{86}}
\newlabel{7.9}{{8.9}{86}}
\newlabel{7.10}{{8.10}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}改进方案}{86}}
\newlabel{7.11}{{8.11}{86}}
\citation{hochreiter1997long}
\citation{hochreiter1997long}
\citation{sutskever2014sequence}
\newlabel{7.12}{{8.12}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}长短时记忆神经网络：LSTM}{87}}
\newlabel{7.14}{{8.14}{87}}
\citation{cho2014learning}
\citation{chung2014empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces LSTM Structure Expamle}}{88}}
\newlabel{fig:7.4}{{8.4}{88}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}门限循环单元：GRU}{88}}
\bibstyle{plain}
\bibdata{Lect}
\newlabel{7.20}{{8.20}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}一个强大的RNN框架：DeepNet}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}一些经典论文基于DeepNet的实验重现}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}进一步的阅读和总结}{89}}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{bengio2009learning}{1}
\bibcite{bengio1994learning}{2}
\bibcite{bishop2006pattern}{3}
\bibcite{cho2014learning}{4}
\bibcite{chung2014empirical}{5}
\bibcite{duchi2011adaptive}{6}
\bibcite{elman1990finding}{7}
\bibcite{gatys2015neural}{8}
\bibcite{glorot2010understanding}{9}
\bibcite{Goodfellow-et-al-2015-Book}{10}
\bibcite{he2015delving}{11}
\bibcite{hinton2012deep}{12}
\bibcite{hinton2002training}{13}
\bibcite{hinton2006reducing}{14}
\bibcite{hochreiter2001gradient}{15}
\bibcite{hochreiter1997long}{16}
\bibcite{hu2015information}{17}
\bibcite{hubel1968receptive}{18}
\bibcite{krizhevsky2012imagenet}{19}
\bibcite{lecun1998gradient}{20}
\bibcite{mackay2003information}{21}
\bibcite{mitchell1998introduction}{22}
\bibcite{principe2000information}{23}
\bibcite{rosenblatt1958perceptron}{24}
\bibcite{rumelhart1988learning}{25}
\bibcite{simard2003best}{26}
\bibcite{sun2014deep}{27}
\bibcite{sutskever2014sequence}{28}
\bibcite{waibel1989phoneme}{29}
\bibcite{welling2007product}{30}
\bibcite{werbos1990backpropagation}{31}
\bibcite{zeiler2012adadelta}{32}
\@writefile{toc}{\contentsline {chapter}{Appendices}{97}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第九章\hspace  {0.3em}}学习理论的统计机理}{97}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.A}一些约定}{97}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.A.1}Annealed Analysis of Gibbs Learning}{97}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces 不同类型的神经网络：a)普适结构，b)全链接吸引神经网络，c)带一个隐层的反馈神经网络，d)单层感知器}}{98}}
\newlabel{fig:app1}{{9.1}{98}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Projection of the input space to the plane spanned by the coupling vectors of teacher and student. Patterns with projection in the shaded region are classified wrongly by student.}}{99}}
\newlabel{fig:app2}{{9.2}{99}}
\newlabel{app1}{{9.1}{100}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Expression of 9.1\hbox {} as a function of $\varepsilon $ for $\alpha =0,1,2,3,4\text  { and }5$ (from top to bottom)}}{101}}
\newlabel{fig:app3}{{9.3}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.A.2}The Annealed Approximation in Statistical Mechanics}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {9.B}The Gardner Analysis}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {9.C}Learning by Minimizing Cost Functions}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {9.D}Noisy Teachers}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {9.E}Variations of Preceptron Learning}{101}}
