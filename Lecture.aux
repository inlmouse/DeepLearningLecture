\relax 
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第一章\hspace  {0.3em}}序论}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1988learning}
\citation{hinton2006reducing}
\citation{hinton2012deep}
\citation{krizhevsky2012imagenet}
\citation{Goodfellow-et-al-2015-Book}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}进一步的阅读和总结}{9}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第二章\hspace  {0.3em}}数学准备知识}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}矢量分析}{11}}
\newlabel{2.1}{{2.1}{11}}
\newlabel{2.2}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}矢量的模}{11}}
\newlabel{2.3}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}矢量的范数}{11}}
\newlabel{2.4}{{2.4}{12}}
\newlabel{2.5}{{2.5}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}矩阵及其基本运算}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}常见的矩阵}{12}}
\newlabel{2.6}{{2.6}{12}}
\newlabel{2.7}{{2.7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}矩阵的范数}{12}}
\newlabel{compatibility}{{2.8}{12}}
\newlabel{2.8}{{2.9}{12}}
\newlabel{UnitaryInvariance}{{2.10}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}导数}{13}}
\newlabel{div}{{2.11}{13}}
\newlabel{2.9}{{2.12}{13}}
\newlabel{2.10}{{2.13}{14}}
\newlabel{divmat}{{2.14}{14}}
\newlabel{innerdivmat}{{2.15}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}常见的向量导数}{14}}
\newlabel{ex1}{{2.16}{14}}
\newlabel{ex2}{{2.17}{14}}
\newlabel{ex3}{{2.18}{14}}
\newlabel{2.11}{{2.19}{14}}
\newlabel{2.12}{{2.20}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}导数法则}{14}}
\newlabel{2.13}{{2.21}{15}}
\newlabel{2.14}{{2.22}{15}}
\newlabel{2.15}{{2.23}{15}}
\newlabel{2.16}{{2.24}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}常用函数}{15}}
\newlabel{2.17}{{2.25}{15}}
\newlabel{2.18}{{2.26}{15}}
\newlabel{2.19}{{2.27}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}logistic/sigmoid函数}{16}}
\newlabel{2.20}{{2.28}{16}}
\newlabel{2.21}{{2.29}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}softmax函数}{16}}
\newlabel{2.22}{{2.30}{16}}
\newlabel{2.23}{{2.31}{16}}
\newlabel{2.24}{{2.32}{16}}
\newlabel{2.27}{{2.35}{17}}
\newlabel{2.30}{{2.38}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Rectifier函数(ReLU)}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}一些练习}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}进一步的阅读和总结}{18}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第三章\hspace  {0.3em}}机器学习概述}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}机器学习概述}{19}}
\newlabel{3.1}{{3.1}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces 机器学习系统示意图}}{19}}
\newlabel{fig:3.1}{{3.1}{19}}
\citation{mitchell1998introduction}
\newlabel{3.2}{{3.2}{20}}
\newlabel{3.3}{{3.3}{20}}
\newlabel{3.4}{{3.4}{20}}
\newlabel{3.5}{{3.5}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Overfit}}{21}}
\newlabel{fig:Regu1}{{3.2}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}补充问题：正则化}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}损失函数}{24}}
\newlabel{3.7}{{3.7}{24}}
\citation{principe2000information}
\newlabel{3.8}{{3.8}{25}}
\newlabel{3.9}{{3.9}{25}}
\newlabel{3.10}{{3.10}{25}}
\newlabel{3.11}{{3.11}{25}}
\newlabel{3.12}{{3.12}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}补充问题：机器学习与信息论的关系}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Some information formulas and their properties as learning measures}}{26}}
\newlabel{fig:3.2}{{3.3}{26}}
\citation{mackay2003information}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The relationship among some measures}}{27}}
\newlabel{fig:3.3}{{3.4}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The relationship among $E,Rej,A,CR$}}{27}}
\newlabel{fig:3.4}{{3.5}{27}}
\citation{hu2015information}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Example}}{28}}
\newlabel{fig:3.5}{{3.6}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}机器学习算法的类型}{29}}
\newlabel{3.13}{{3.13}{29}}
\newlabel{3.14}{{3.14}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}机器学习中的一些基本概念}{30}}
\newlabel{3.15}{{3.15}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}参数学习方法}{31}}
\newlabel{3.16}{{3.16}{31}}
\newlabel{3.17}{{3.17}{32}}
\newlabel{3.19}{{3.19}{32}}
\newlabel{3.20}{{3.21}{32}}
\citation{rumelhart1988learning}
\citation{duchi2011adaptive}
\citation{zeiler2012adadelta}
\newlabel{3.21}{{3.22}{33}}
\newlabel{3.22}{{3.23}{33}}
\newlabel{3.23}{{3.24}{33}}
\newlabel{3.24}{{3.25}{33}}
\newlabel{3.25}{{3.26}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}线性回归}{34}}
\newlabel{3.26}{{3.27}{34}}
\newlabel{3.27}{{3.28}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}线性分类}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}二类分类}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}多类线性分类}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}评价方法}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}进一步的阅读和总结}{34}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {第四章\hspace  {0.3em}}感知器}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}二类感知器}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}感知器学习算法}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}线性感知器收敛性证明}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}多类感知器}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}多类感知器收敛性证明}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}投票感知器}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}进一步的阅读和总结}{35}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {chapter}{\numberline {第五章\hspace  {0.3em}}人工神经网络}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}神经元}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}激活函数}{38}}
\citation{glorot2010understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Neuron}}{39}}
\newlabel{fig:5.1}{{5.1}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}激活函数的表达能力}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Activation Function}}{40}}
\newlabel{fig:5.2}{{5.2}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces $s$-Volume}}{43}}
\newlabel{fig:5.3}{{5.3}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}前馈神经网络}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}前馈计算}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}反向传播算法}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}梯度消失问题}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}训练方法}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}一些经验}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}进一步的阅读和总结}{43}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {第六章\hspace  {0.3em}}受限波尔兹曼机RBM}{45}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Roadmap}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Notations}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Energy-Based Models}{46}}
\newlabel{add1.1}{{6.1}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces An Simple Example of Energy-Based Models}}{47}}
\newlabel{fig:add1}{{6.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}An Simple Example of Energy-Based Models}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces $Energy(a,b)$ and the probability for each configuration}}{47}}
\newlabel{tab:Energy}{{6.1}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Introducing Hidden Variables}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Model}}{48}}
\newlabel{fig:add2}{{6.2}{48}}
\newlabel{add1.2}{{6.2}{48}}
\newlabel{add1.3}{{6.3}{48}}
\newlabel{add1.4}{{6.4}{48}}
\newlabel{add1.5}{{6.5}{48}}
\newlabel{add1.6}{{6.6}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Gradient Learning of Energy-based Models}}{49}}
\newlabel{fig:add3}{{6.3}{49}}
\newlabel{add1.7}{{6.7}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Gradient Learning of Energy-based Models}{49}}
\newlabel{add1.8}{{6.8}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Boltzmann Machines}{50}}
\newlabel{BME}{{6.9}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Gradient Learning of Boltzmann Machines}{50}}
\newlabel{LLG}{{6.10}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Gibbs Sampling for Conditional Probability}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Gibbs Sampling for Boltzmann Machines}{51}}
\citation{hinton2002training}
\citation{welling2007product}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Restricted Boltzmann Machines}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Gibbs Sampling for Restricted Boltzmann Machines}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {6.13}Contrastive Divergence}{53}}
\newlabel{CDk}{{6.11}{54}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces k-step contrastive divergence}}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {6.14}Gibbs Sampling和Markov Chain以及MCMC的关系}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {6.15}CD Algorithm是如何对原来的分布$p$ 进行优化的}{56}}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{hubel1968receptive}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {chapter}{\numberline {第七章\hspace  {0.3em}}卷积神经网络CNN}{59}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}卷积}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}一维场合}{59}}
\newlabel{6.1}{{7.1}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Full Connection Layer and Convolutional Layer}}{60}}
\newlabel{fig:6.1}{{7.1}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}二维场合}{60}}
\newlabel{6.2}{{7.2}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}卷积层：用卷积代替全链接}{60}}
\newlabel{6.2}{{7.3}{61}}
\newlabel{6.5}{{7.5}{61}}
\newlabel{6.6}{{7.6}{61}}
\newlabel{6.7}{{7.7}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The mapping relationship of 2-D convolutional layer}}{62}}
\newlabel{fig:6.2}{{7.2}{62}}
\newlabel{6.8}{{7.8}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces 2-D convolutional layer}}{63}}
\newlabel{fig:6.3}{{7.3}{63}}
\newlabel{6.9}{{7.9}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}子采样层：池化}{63}}
\newlabel{6.10}{{7.10}{63}}
\citation{lecun1998gradient}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Net Structure of LeNet-5}}{64}}
\newlabel{fig:6.4}{{7.4}{64}}
\newlabel{6.12}{{7.12}{64}}
\newlabel{6.14}{{7.14}{64}}
\newlabel{6.15}{{7.15}{64}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}CNN示例：LeNet-5}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Connection table of LeNet-5's C3 layer}}{65}}
\newlabel{fig:6.5}{{7.5}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}梯度计算}{66}}
\newlabel{6.16}{{7.16}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}卷积层的梯度}{66}}
\newlabel{6.18}{{7.18}{66}}
\newlabel{6.22}{{7.22}{67}}
\newlabel{6.24}{{7.24}{67}}
\newlabel{6.25}{{7.25}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}子采样层的梯度}{67}}
\newlabel{6.26}{{7.26}{67}}
\newlabel{6.27}{{7.27}{67}}
\newlabel{6.30}{{7.30}{68}}
\newlabel{6.31}{{7.31}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}一个强大的CNN框架：CAFFE}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Caffe的特点}{68}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe相对与其他DL框架的优点和缺点}{68}}
\@writefile{toc}{\contentsline {subsubsection}{Caffe代码层次}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}更进一步的特点}{69}}
\@writefile{toc}{\contentsline {subsubsection}{Blob}{69}}
\@writefile{toc}{\contentsline {subsubsection}{Layer}{69}}
\@writefile{toc}{\contentsline {subsubsection}{Net}{71}}
\@writefile{toc}{\contentsline {subsubsection}{Solver}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}一些关于CNN的技巧}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Data Augmentation}{72}}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Pre-Processing}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Initializations}{75}}
\citation{he2015delving}
\citation{he2015delving}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}During Training}{77}}
\citation{gatys2015neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.5}Activation Functions}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.6}Regularizations}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.7}Insights from Figures}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.8}Ensemble}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.9}Miscellaneous}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}一些经典论文基于CAFFE的实验重现}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}A Neural Algorithm of Artistic Style}{78}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}进一步的阅读和总结}{79}}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Images that combine the content of a photograph with the style of several well-known artworks.}}{80}}
\newlabel{fig:6.6}{{7.6}{80}}
\citation{waibel1989phoneme}
\citation{werbos1990backpropagation}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\citation{hochreiter2001gradient}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {chapter}{\numberline {第八章\hspace  {0.3em}}递归神经网络RNN}{81}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{7.1}{{8.1}{81}}
\citation{elman1990finding}
\citation{werbos1990backpropagation}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces RNN}}{82}}
\newlabel{fig:7.1}{{8.1}{82}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}简单的递归网络}{82}}
\newlabel{7.2}{{8.2}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}梯度}{82}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Simple RNN expanded by time}}{83}}
\newlabel{fig:7.2}{{8.2}{83}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces RNN expanded by time}}{83}}
\newlabel{fig:7.3}{{8.3}{83}}
\newlabel{7.3}{{8.3}{83}}
\newlabel{7.5}{{8.5}{83}}
\newlabel{7.6}{{8.6}{83}}
\newlabel{7.8}{{8.8}{84}}
\newlabel{7.9}{{8.9}{84}}
\newlabel{7.10}{{8.10}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}改进方案}{84}}
\newlabel{7.11}{{8.11}{84}}
\citation{hochreiter1997long}
\citation{hochreiter1997long}
\citation{sutskever2014sequence}
\newlabel{7.12}{{8.12}{85}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}长短时记忆神经网络：LSTM}{85}}
\newlabel{7.14}{{8.14}{85}}
\citation{cho2014learning}
\citation{chung2014empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces LSTM Structure Expamle}}{86}}
\newlabel{fig:7.4}{{8.4}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}门限循环单元：GRU}{86}}
\bibstyle{plain}
\bibdata{Lect}
\newlabel{7.20}{{8.20}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}一个强大的RNN框架：DeepNet}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}一些经典论文基于DeepNet的实验重现}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}进一步的阅读和总结}{87}}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{bengio2009learning}{1}
\bibcite{bengio1994learning}{2}
\bibcite{bishop2006pattern}{3}
\bibcite{cho2014learning}{4}
\bibcite{chung2014empirical}{5}
\bibcite{duchi2011adaptive}{6}
\bibcite{elman1990finding}{7}
\bibcite{gatys2015neural}{8}
\bibcite{glorot2010understanding}{9}
\bibcite{Goodfellow-et-al-2015-Book}{10}
\bibcite{he2015delving}{11}
\bibcite{hinton2012deep}{12}
\bibcite{hinton2002training}{13}
\bibcite{hinton2006reducing}{14}
\bibcite{hochreiter2001gradient}{15}
\bibcite{hochreiter1997long}{16}
\bibcite{hu2015information}{17}
\bibcite{hubel1968receptive}{18}
\bibcite{krizhevsky2012imagenet}{19}
\bibcite{lecun1998gradient}{20}
\bibcite{mackay2003information}{21}
\bibcite{mitchell1998introduction}{22}
\bibcite{principe2000information}{23}
\bibcite{rosenblatt1958perceptron}{24}
\bibcite{rumelhart1988learning}{25}
\bibcite{sutskever2014sequence}{26}
\bibcite{waibel1989phoneme}{27}
\bibcite{welling2007product}{28}
\bibcite{werbos1990backpropagation}{29}
\bibcite{zeiler2012adadelta}{30}
